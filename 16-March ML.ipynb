{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58985023-1089-46f3-ba58-57f7aded321f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'overfitting simply means that the learning model is far too dependent on training data\\nwhile underfitting means that the model has a poor relationship with the training data.\\nIdeally, both of these should not exist in models, but they usually are hard to eliminate.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No.1 :\n",
    "\"\"\"overfitting simply means that the learning model is far too dependent on training data\n",
    "while underfitting means that the model has a poor relationship with the training data.\n",
    "Ideally, both of these should not exist in models, but they usually are hard to eliminate.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2f4f4a3-82dd-4771-9aed-1365c002f80e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here we will discuss possible options to prevent overfitting, which helps improve the model performance.\\nTrain with more data. \\nData augmentation. \\nAddition of noise to the input data. \\nFeature selection. \\nCross-validation. \\nSimplify data. \\nRegularization.\\nEnsembling.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No.2 :\n",
    "\"\"\"Here we will discuss possible options to prevent overfitting, which helps improve the model performance.\n",
    "Train with more data. \n",
    "Data augmentation. \n",
    "Addition of noise to the input data. \n",
    "Feature selection. \n",
    "Cross-validation. \n",
    "Simplify data. \n",
    "Regularization.\n",
    "Ensembling.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4dd87fe-1a94-401a-aacc-f544a95974b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Underfitting is a scenario in data science where a data model is unable to capture the relationship between the \\ninput and output variables accurately, generating a high error rate on both the training set and unseen data.\\n\\nUnderfitting occurs when a model is too simple — informed by too few features or regularized too much — \\nwhich makes it inflexible in learning from the dataset.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No.3 :\n",
    "\"\"\"Underfitting is a scenario in data science where a data model is unable to capture the relationship between the \n",
    "input and output variables accurately, generating a high error rate on both the training set and unseen data.\n",
    "\n",
    "Underfitting occurs when a model is too simple — informed by too few features or regularized too much — \n",
    "which makes it inflexible in learning from the dataset.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "914d6252-583b-4978-864e-08b532644ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'“Bias and variance are complements of each other” The increase of one will result in the decrease of the other and vice versa.\\nHence, finding the right balance of values is known as the Bias-Variance Tradeoff.\\nBias and variance are inversely connected. It is impossible to have an ML model with a low bias and a low variance.\\nWhen a data engineer modifies the ML algorithm to better fit a given data set, \\nit will lead to low bias—but it will increase variance.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No.4 :\n",
    "\"\"\"“Bias and variance are complements of each other” The increase of one will result in the decrease of the other and vice versa.\n",
    "Hence, finding the right balance of values is known as the Bias-Variance Tradeoff.\n",
    "Bias and variance are inversely connected. It is impossible to have an ML model with a low bias and a low variance.\n",
    "When a data engineer modifies the ML algorithm to better fit a given data set, \n",
    "it will lead to low bias—but it will increase variance.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eaf41f08-c8b0-4ca9-a8ba-49fe84186a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are a few ways we can understand how to \"diagnose\" underfitting and overfitting.  \\n\\nUnderfitting occurs when your model produces accurate but inaccurate predictions at first.\\nIn this scenario, the training error is substantial, as is the validation/test error.  \\n\\nOverfitting occurs when your model fails to generate correct predictions.\\nThe training error is relatively modest in this example, but the validation/test error is highly significant.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No.5 :\n",
    "\"\"\"There are a few ways we can understand how to \"diagnose\" underfitting and overfitting.  \n",
    "\n",
    "Underfitting occurs when your model produces accurate but inaccurate predictions at first.\n",
    "In this scenario, the training error is substantial, as is the validation/test error.  \n",
    "\n",
    "Overfitting occurs when your model fails to generate correct predictions.\n",
    "The training error is relatively modest in this example, but the validation/test error is highly significant.\"\"\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "191d8b12-6830-47e6-b13f-d5d33650703d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bias is the amount that a model’s prediction differs from the target value, compared to the training data. \\nVariance indicates how much the estimate of the target function will alter if different training data were used.\\nvariance describes how much a random variable differs from its expected value.\\nA model with high variance may represent the data set accurately but could lead to overfitting to noisy or otherwise unrepresentative training data.\\nIn comparison, a model with high bias may underfit the training data due to a simpler model that overlooks regularities in the data.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No.6 :\n",
    "\"\"\"Bias is the amount that a model’s prediction differs from the target value, compared to the training data. \n",
    "Variance indicates how much the estimate of the target function will alter if different training data were used.\n",
    "variance describes how much a random variable differs from its expected value.\n",
    "A model with high variance may represent the data set accurately but could lead to overfitting to noisy or otherwise unrepresentative training data.\n",
    "In comparison, a model with high bias may underfit the training data due to a simpler model that overlooks regularities in the data.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0553a1a-743e-4ec5-85ce-8b0452911383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Regularization refers to techniques that are used to calibrate machine learning models in order to minimize the \\nadjusted loss function and prevent overfitting or underfitting.\\nRegularization comes into play and shrinks the learned estimates towards zero.\\nIn other words, it tunes the loss function by adding a penalty term, that prevents excessivefluctuation of the coefficients. \\nThereby, reducing the chances of overfitting.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No.7 :\n",
    "\"\"\"Regularization refers to techniques that are used to calibrate machine learning models in order to minimize the \n",
    "adjusted loss function and prevent overfitting or underfitting.\n",
    "Regularization comes into play and shrinks the learned estimates towards zero.\n",
    "In other words, it tunes the loss function by adding a penalty term, that prevents excessivefluctuation of the coefficients. \n",
    "Thereby, reducing the chances of overfitting.\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
